{"nbformat_minor": 2, "cells": [{"source": "## Sample Pyspark Code Syntax\n\n#### A jupyter notebook for all your sample pyspark syntax", "cell_type": "markdown", "metadata": {}}, {"source": " ", "cell_type": "markdown", "metadata": {}}, {"source": "#### Initiate spark session", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%configure -\n{\n    \"executorMemory\": \"20G\",\n    \"numExecutors\":10, \n    \"executorCores\":10, \n    \"driverMemory\": \"4G\", \n    \"name\": \"SAMPLE PYSPARK APP NAME\"\n}", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### Basic imports", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.sql import SQLContext\nfrom pyspark.sql import functions as sf\nfrom pyspark.sql.types import BooleanType, StringType, IntegerType, DateType, StructField, StructType\nfrom pyspark.sql.functions import col, when, concat_ws", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### Print to notebook", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "some_var = \"abcd\"\nprint(\"Sample line to printed:\", some_var)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### Create your desired date range here", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "path = cluster_path + all_events_path_18_19 + \"2019-01-01*\"", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### Read Parquet/CSV", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "columns_to_read = [\"event\",\"gaid\",\"media_id\",\"duration_seconds\", \"lr_age\"]\ndata = spark.read.parquet(path)[columns_to_read]", "outputs": [], "metadata": {"collapsed": false}}, {"source": "#### Filter", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "data = data.filter(data.event==\"mediaReady\")\ndata = data.filter(data.duration_seconds < 84000)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### Filter with multiple conditions", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "## OR\ndata = data.filter((data.gaid.isNotNull()) | (data.lr_age.isNotNull()) \n\n## AND                    \ndata = data.filter((data.duration_seconds > 0 & data.duration_seconds < 84000)                    ", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### Filter by an user defined criterion (function)", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "def filter_function(filter_column_1, filter_column_2):\n\nudf_null_filter = sf.udf(filter_function, BooleanType())\ncomplete_media_data = complete_data.filter(udf_null_filter(\"event\",\"duration_seconds\"))\n", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### Select columns", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "data = complete_data.select(['gaid', 'media_id','duration_seconds'])", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### Distinct of a dataframe", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "distinct_data = data.distinct()\n\nselective_distinct_data = data.dropDuplicates(['gaid','lr_age'])", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### Check the data frame strucutre alternative to str(df) in R", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "data.printSchema()", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### Rename column", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "data = data.withColumnRenamed(\"from_column_name\",\"to_column_name\")", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### Aggregations", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "data = data.groupBy(\"gaid\",\"media_id\") \\\n                             .agg({\"duration_seconds\":\"sum\"}) \\\n                             .withColumnRenamed(\"sum(duration_seconds)\",\"total_wt_per_media_id\")", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### Union of dataframes", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "all_data = data_part_1.union(data_part_2)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### Create SQL table ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "project_prefix = \"001_SAMPLE_PROJECT_\"\ntable_name = project_prefix + \"SAMPLE_TABLE_NAME\"\n\ndata.createOrReplaceTempView(\"data_temp_view\") \nspark.sql(\"CREATE TABLE {0} AS SELECT * FROM data_temp_view\".format(table_name))", "outputs": [], "metadata": {"collapsed": true}}, {"source": "#### Write to CSV", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "csv_write_path = \"/tmp/YOUR_NAME_HERE/FILE_NAME_HERE\"\nnumber_of_partitions = 1\n\ndata.coalesce(number_of_partitions).write.format('com.databricks.spark.csv').save(csv_data_path)", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}